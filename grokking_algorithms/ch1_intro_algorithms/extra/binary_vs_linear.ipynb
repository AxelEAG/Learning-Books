{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eaf9d38",
   "metadata": {},
   "source": [
    "## **Binary vs Linear Search**\n",
    "\n",
    "**Linear search** is the simplest search. It just goes through every single element until finding the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f6aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant modules\n",
    "\n",
    "from random import randint, sample, random\n",
    "from typing import TypedDict, Dict, List, Optional, Callable, Any\n",
    "from time import perf_counter_ns as t_ns\n",
    "from binary_search import binary_search\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa210c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple implementation of linear search\n",
    "def linear_search(ls: list, target: int) -> Optional[int]:\n",
    "    for i, num in enumerate(ls):\n",
    "        if num == target:\n",
    "            return i\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240eaa2c",
   "metadata": {},
   "source": [
    "### **Analyzing Complexity**\n",
    "\n",
    "Binary search halves the search range on every step, whereas linear search only decreases the range by one on every step. So, in theory, binary search should be O(log n), while linear search O(n). In this experiment, I'll set out to showcase this. \n",
    "\n",
    "This analysis however, is for the worst case. In practice, one may see other results - such as best case, or, when aggregated, an average case of a given sample. So it is worthwhile thinking of these.\n",
    "\n",
    "For binary search: \n",
    "* **Best case**    - Element is at the middle. So it needs 1 operation.\n",
    "* **Worst case**   - Element is where the target happens to be the last one checked (or not present). It needs log n.\n",
    "* **Average case** - Everything else is in between these two. On average, this leads to ~ (log n) - 1. Why?\n",
    "\n",
    "For linear search:\n",
    "* **Best case**    - Element is at the beginning. So it needs 1 operation.\n",
    "* **Worst case**   - Element is at the end (or not present). It needs n operations\n",
    "* **Average case** - Everything else is in between the two. As it's evenly distributed, this leads to ~ n / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ca39d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explain why average case of binary search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600c284",
   "metadata": {},
   "source": [
    "would be if the element is at the exact middle (so it needs 1 operation), while the worst is when it's at one of those tricky spots where the target happens to be the last one checked or it's not on the list (so it needs log n operations). \n",
    "Everything else is in between these two values, the average being roughly (log n) - 1. An intuition for this is: if we picture the list as a tree, most elements are found above the deepest level, so on average the number of comparisons is slightly smaller than the height of the tree (log n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec9373",
   "metadata": {},
   "source": [
    "### **Experimental Set Up**\n",
    "\n",
    "The base set up will be as follows: \n",
    "1. For every size in _sizes_, generate a list as a fixed sorted sequence from 1 to _size_.  \n",
    "2. Then, select a target and search it with both methods. \n",
    "3. Repeat this _attempts_ amount of times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c53f2",
   "metadata": {},
   "source": [
    "##### **Timing the Searches**\n",
    "\n",
    "In this case, I use a [wrapper function](./extra/wrapper_functions.ipynb) to time the different searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f62538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper that returns the time a function takes to run\n",
    "def timed(func: Callable[..., Any], *args, **kwargs) -> tuple[Any, int]:\n",
    "    \"\"\"\n",
    "    Runs a function and returns (result, elapsed_time_ns).\n",
    "    \"\"\"\n",
    "    start = t_ns()\n",
    "    result = func(*args, **kwargs)\n",
    "    elapsed = t_ns() - start\n",
    "    return result, elapsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b840b03",
   "metadata": {},
   "source": [
    "#### **Defining Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43221781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeResult(TypedDict):                                            # size : {\"times\": [], \"indices\": []}\n",
    "    times:   List[float]\n",
    "    indices: List[int]\n",
    "\n",
    "class SearchResults(TypedDict):                                         # search: {size: SizeResult}\n",
    "    binary:  Dict[int, SizeResult]\n",
    "    linear:  Dict[int, SizeResult]\n",
    "    targets: Dict[int, List[int]]\n",
    "\n",
    "class ExpResults(TypedDict):\n",
    "    hit_rate: SearchResults\n",
    "\n",
    "def make_size_result():\n",
    "    return {\"times\": [], \"indices\": []}             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a46cf7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [8, 32, 128, 1024, 16384, 131072]                               # sizes = 2**n for n = 3, 5, 7, 10, 17. Each 1 order of magnitude bigger\n",
    "hit_rates = [1]                                                 # ratio of targets inside list (hit) vs outside (miss)\n",
    "attempts = 100                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c389e46",
   "metadata": {},
   "source": [
    "##### **Running the Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(sizes: List[int], hit_rates: List[int], attempts: int) -> ExpResults:\n",
    "    exp_results = {}\n",
    "    for rate in hit_rates:\n",
    "        results: SearchResults = {\n",
    "            \"binary\": defaultdict(make_size_result),\n",
    "            \"linear\": defaultdict(make_size_result),\n",
    "            \"targets\": defaultdict(list)\n",
    "        }\n",
    "\n",
    "        # Run experiment for every size\n",
    "        for size in sizes:\n",
    "            nums = list(range(1, size + 1))\n",
    "\n",
    "            # Check first, middle, last, and a miss.\n",
    "            indices = [1, size // 2, size - 1, -1]\n",
    "\n",
    "            # Add a combination of hits and misses in proportion to hit rate\n",
    "            hits = int(rate*attempts)\n",
    "            misses = attempts - hits\n",
    "            if misses: indices.extend([-1]*misses)\n",
    "            if hits: indices.extend([randint(0, size - 1) for _ in range(hits)])\n",
    "\n",
    "            # Run searches 'attempts' amount of times\n",
    "            for idx in indices:\n",
    "                if idx == -1: target = -1\n",
    "                else: target = nums[idx]\n",
    "\n",
    "                lin_i, lin_t = timed(linear_search, nums, target)\n",
    "                bin_i, bin_t = timed(binary_search, nums, target)\n",
    "\n",
    "                # Check validity - shouldn't be necessary!\n",
    "                if lin_i != bin_i: # Expects miss\n",
    "                    if idx == -1:\n",
    "                        print(f\"Expected a miss on both. Got index {lin_i} for linear and {bin_i} for binary.\")\n",
    "                    else:\n",
    "                        print(f\"Expected index {idx}. Got index {lin_i} for linear and {bin_i} for binary.\")\n",
    "\n",
    "                # Save data\n",
    "                results[\"linear\"][size][\"indices\"].append(lin_i)\n",
    "                results[\"linear\"][size][\"times\"].append(lin_t)\n",
    "                results[\"binary\"][size][\"indices\"].append(bin_i)\n",
    "                results[\"binary\"][size][\"times\"].append(bin_t)\n",
    "                results[\"targets\"][size].append(idx)\n",
    "            \n",
    "\n",
    "        exp_results[rate] = results\n",
    "    \n",
    "    return exp_results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "849573a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results = run_experiment(sizes, hit_rates, attempts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4c2e1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for size 8: Linear= 0.27 Binary= 0.28 [microseconds]\n",
      "Times for Linear: first= 1300.00, middle= 500.00, last= 400.00, miss= 800.00\n",
      "Times for Binary: first= 1100.00, middle= 500.00, last= 400.00, miss= 700.00\n",
      "Average time for size 32: Linear= 0.52 Binary= 0.33 [microseconds]\n",
      "Times for Linear: first= 300.00, middle= 500.00, last= 800.00, miss= 900.00\n",
      "Times for Binary: first= 400.00, middle= 400.00, last= 400.00, miss= 400.00\n",
      "Average time for size 128: Linear= 1.44 Binary= 0.44 [microseconds]\n",
      "Times for Linear: first= 300.00, middle= 1400.00, last= 2500.00, miss= 2500.00\n",
      "Times for Binary: first= 500.00, middle= 500.00, last= 500.00, miss= 500.00\n",
      "Average time for size 1024: Linear= 11.34 Binary= 0.78 [microseconds]\n",
      "Times for Linear: first= 200.00, middle= 10900.00, last= 20900.00, miss= 21500.00\n",
      "Times for Binary: first= 1200.00, middle= 1500.00, last= 1000.00, miss= 1000.00\n",
      "Average time for size 16384: Linear= 177.21 Binary= 1.21 [microseconds]\n",
      "Times for Linear: first= 300.00, middle= 175800.00, last= 365300.00, miss= 404500.00\n",
      "Times for Binary: first= 1100.00, middle= 1900.00, last= 1600.00, miss= 1500.00\n",
      "Average time for size 131072: Linear= 1471.61 Binary= 2.49 [microseconds]\n",
      "Times for Linear: first= 1500.00, middle= 1400500.00, last= 2875300.00, miss= 2861700.00\n",
      "Times for Binary: first= 4900.00, middle= 4400.00, last= 2100.00, miss= 2500.00\n"
     ]
    }
   ],
   "source": [
    "for size in sizes:\n",
    "    times_l = exp_results[1][\"linear\"][size][\"times\"]\n",
    "    times_b = exp_results[1][\"binary\"][size][\"times\"]\n",
    "\n",
    "    avg_b = sum(times_b) / (attempts * 10**3) # nanosecond to microsecond\n",
    "    avg_l = sum(times_l) / (attempts * 10**3)\n",
    "    print(f\"Average time for size {size}: Linear={avg_l: .2f} Binary={avg_b: .2f} [microseconds]\")\n",
    "    print(f\"Times for Linear: first={times_l[0]: .2f}, middle={times_l[1]: .2f}, last={times_l[2]: .2f}, miss={times_l[3]: .2f}\")\n",
    "    print(f\"Times for Binary: first={times_b[0]: .2f}, middle={times_b[1]: .2f}, last={times_b[2]: .2f}, miss={times_b[3]: .2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e573fc",
   "metadata": {},
   "source": [
    "As expected, binary wins on all but first, as for linear it's the first it checks, while for binary it's among the last.\n",
    "Also one can notice similar performance for the middle on the first 2 as there's so few elements, they do a similar amount of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1ff5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a685e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "5.0\n",
      "7.0\n",
      "10.0\n",
      "14.0\n",
      "17.0\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "for size in sizes:\n",
    "    print(log2(size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
